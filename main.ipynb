{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate + Create Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aml-aks-poc loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Environment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Experiment\n",
    "import os\n",
    "\n",
    "# from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ds = ws.get_default_datastore()\n",
    "print(ws.name, \"loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "folder_training_script = './jobcode'\n",
    "os.makedirs(folder_training_script, exist_ok=True)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data by using get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Uploading ./jobsdata\\jobs.csv\n",
      "Uploaded ./jobsdata\\jobs.csv, 1 files out of an estimated total of 2\n",
      "Uploading ./jobsdata\\jobs_old.csv\n",
      "Uploaded ./jobsdata\\jobs_old.csv, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n",
      "Uploaded Jobs Data\n"
     ]
    }
   ],
   "source": [
    "ds.upload(src_dir='./jobsdata', target_path='jobsdata', overwrite=True, show_progress=True)\n",
    "\n",
    "print('Uploaded Jobs Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azureml.data._dataset_deprecation:\"Datastore.upload\" is deprecated after version 1.0.69. Please use \"Dataset.File.upload_directory\" to upload your files             from a local directory and create FileDataset in single method call. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./jobcode\\train.py\n",
      "Uploaded ./jobcode\\train.py, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_8bc3699dab9c4c7a9f246e609a9a4640"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.upload(src_dir='./jobcode', target_path='jobcode', overwrite=True, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute target created\n"
     ]
    }
   ],
   "source": [
    "# Naming the cluster and setting minimal and maximal number of nodes \n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 1)\n",
    "\n",
    "# Choosing environment variables \n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "provisioning_config = AmlCompute.provisioning_configuration(\n",
    "    vm_size = vm_size, min_nodes = min_nodes, max_nodes = max_nodes)\n",
    "\n",
    "# Creating the cluster\n",
    "compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "print('Compute target created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./jobcode/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $folder_training_script/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from azureml.core import Run\n",
    "# from utils import load_data\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# let user feed in 2 parameters, the dataset to mount or download, and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--max-depth', type=int, dest='max_depth', default=5, help='max depth')\n",
    "args = parser.parse_args()\n",
    "\n",
    "###\n",
    "data_folder = os.path.join(args.data_folder, 'jobsdata')\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "job_data = pd.read_csv(os.path.join(data_folder, 'jobs.csv'))\n",
    "\n",
    "                        \n",
    "X = job_data.drop(columns =[\"quality\"])\n",
    "y = job_data[\"quality\"]\n",
    "\n",
    "clf = DecisionTreeRegressor(random_state=0,max_depth = args.max_depth)\n",
    "rmse= np.mean(np.sqrt(-cross_val_score(clf, X, y, scoring=\"neg_mean_squared_error\", cv = 5)))\n",
    "print('RMSE is', rmse)\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "run.log('max depth', np.float(args.max_depth))\n",
    "run.log('rmse', np.float(rmse))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "clf.fit(X,y)\n",
    "# file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=clf, filename='outputs/job_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azureml.core.environment:'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job-experiment-env-5 defined.\n"
     ]
    }
   ],
   "source": [
    "job_env = Environment(\"job-experiment-env-5\")\n",
    "job_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "job_env.docker.enabled = True # Docker container\n",
    "\n",
    "job_packages = CondaDependencies.create(conda_packages=['scikit-learn', \"numpy\", \"pandas\", \"joblib\"])\n",
    "\n",
    "# Add the dependencies to the environment\n",
    "job_env.python.conda_dependencies = job_packages\n",
    "\n",
    "print(job_env.name, 'defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering env to Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"assetId\": \"azureml://locations/eastus2/workspaces/6f7360e0-658a-4233-8141-019587c2881d/environments/job-experiment-env-5/versions/1\",\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20221010.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"buildContext\": null,\n",
       "        \"enabled\": true,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"job-experiment-env-5\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.8.13\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults~=1.47.0\"\n",
       "                    ]\n",
       "                },\n",
       "                \"scikit-learn\",\n",
       "                \"numpy\",\n",
       "                \"pandas\",\n",
       "                \"joblib\"\n",
       "            ],\n",
       "            \"name\": \"project_environment\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"1\"\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azureml.train.estimator._estimator:'Estimator' is deprecated. Please use 'ScriptRunConfig' from 'azureml.core.script_run_config' with your own defined environment or an Azure ML curated environment.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--max-depth': 10\n",
    "}\n",
    "\n",
    "registered_env = Environment.get(ws, 'job-experiment-env-5')\n",
    "\n",
    "# Create an estimator\n",
    "estimator = Estimator(source_directory=folder_training_script,\n",
    "                      script_params=script_params,\n",
    "                      compute_target = compute_target, # Run the experiment on the remote compute target\n",
    "                      environment_definition = registered_env,\n",
    "                      entry_script='train.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Jobs (Experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created\n"
     ]
    }
   ],
   "source": [
    "#Create an experiment\n",
    "experiment = Experiment(workspace = ws, name = \"job_expt\")\n",
    "\n",
    "print('Experiment created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Experiment with Estimator Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n",
      "WARNING:root:If 'arguments' has been provided here and arguments have been specified in 'run_config', 'arguments' provided in ScriptRunConfig initialization will take precedence.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>job_expt</td><td>job_expt_1668252103_63a04a17</td><td>azureml.scriptrun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/job_expt_1668252103_63a04a17?wsid=/subscriptions/cce65e11-e6d2-4fb0-a5b1-bc2013f9a6b5/resourcegroups/aml-aks-poc/workspaces/aml-aks-poc&amp;tid=8ba6fca0-8377-468c-bea6-0097e950bc13\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: job_expt,\n",
       "Id: job_expt_1668252103_63a04a17,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(config=estimator)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_model\tjob_model:1\t1\n"
     ]
    }
   ],
   "source": [
    "model = run.register_model(model_name='job_model',\n",
    "                           model_path='outputs/job_model.pkl',\n",
    "                           tags = {'area': \"jobs\", 'type': \"sklearn\"},\n",
    "                           description = \"salary prediction\")\n",
    "\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./jobcode/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $folder_training_script/score.py\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the registered model file and load it\n",
    "    model_path = Model.get_model_path('job_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    log_txt = 'Data:' + str(data) + ' - Predictions:' + str(predictions)\n",
    "    print(log_txt)\n",
    "    # Return the predictions as any JSON serializable format\n",
    "    return predictions.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dependency info in ./jobcode/env.yml\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Add the dependencies for your model\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "myenv.add_conda_package(\"numpy\")\n",
    "myenv.add_conda_package(\"joblib\")\n",
    "# myenv.add_conda_package(\"json\")\n",
    "# myenv.add_conda_package(\"azure-ml-api-sdk\")\n",
    "\n",
    "# Save the environment config as a .yml file\n",
    "env_file = './jobcode/env.yml'\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                              source_directory = './jobcode',\n",
    "                                              entry_script=\"score.py\",\n",
    "                                              conda_file=\"env.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST, vm_size=\"standard_d11\")\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.compute import ComputeTarget, AksCompute\n",
    "# from azureml.core.webservice import AksWebservice, Webservice\n",
    "# from azureml.core import Model\n",
    "\n",
    "# cluster_name = 'aks-cluster'\n",
    "# aks_target = AksCompute(ws, \"cpucluster\")\n",
    "\n",
    "# deployment_config = AksWebservice.deploy_configuration(cpu_cores=1, memory_gb=1, autoscale_enabled=True, autoscale_target_utilization=30, autoscale_max_replicas=3, autoscale_min_replicas=1)\n",
    "\n",
    "# service = Model.deploy(ws, \"myservice\", [model], classifier_inference_config, deployment_config, aks_target)\n",
    "# service.wait_for_deployment(show_output=True)\n",
    "# print(service.state)\n",
    "# print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                              memory_gb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model to AKS Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vik\\AppData\\Local\\Temp\\ipykernel_12464\\2746030832.py:4: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoint-sdk-v2 /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration. \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  service = Model.deploy(workspace=ws,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-11-12 17:18:08+05:30 Creating Container Registry if not exists.\n",
      "2022-11-12 17:18:08+05:30 Registering the environment.\n",
      "2022-11-12 17:18:09+05:30 Building image..\n",
      "2022-11-12 17:28:27+05:30 Creating resources in AKS.\n",
      "2022-11-12 17:28:27+05:30 Submitting deployment to compute.\n",
      "2022-11-12 17:28:27+05:30 Checking the status of deployment job-service..\n",
      "2022-11-12 17:31:47+05:30 Checking the status of inference endpoint job-service.\n",
      "Succeeded\n",
      "AKS service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = ws.models['job_model']\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'job-service',\n",
    "                       models = [model],\n",
    "                       inference_config = classifier_inference_config,\n",
    "                       deployment_config = classifier_deploy_config,\n",
    "                       deployment_target = production_cluster)\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service.update(enable_app_insights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key, secondary_key = service.get_keys()\n",
    "primary_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Cred from Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = r\"C:\\Users\\VIGNEN\\Documents\\Azure POC\\Passwords.csv\"\n",
    "\n",
    "rows = []\n",
    "fields = []\n",
    "\n",
    "with open(filename, 'r') as csvfile:\n",
    "\tcsvreader = csv.reader(csvfile)\n",
    "\t\n",
    "\tfields = next(csvreader)\n",
    "\n",
    "\tfor row in csvreader:\n",
    "\t\trows.append(row)\n",
    "\n",
    "endpoint = rows[0][0]\n",
    "primary_key = rows[0][1]\n",
    "sqlPassword = rows[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "server = 'aml-aks-poc.database.windows.net'\n",
    "database = 'aks-aml-poc-inputdb'\n",
    "username = 'viknhat'\n",
    "# sqlPassword = '*********'\n",
    "driver = '{ODBC Driver 17 for SQL Server}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Prediction Data from Azure SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.6, 1.58, 0.0, 2.1, 0.137, 5.0, 9.0, 0.99476, 3.5, 0.4, 10.9], [8.4, 0.635, 0.36, 2.0, 0.089, 15.0, 55.0, 0.99745, 3.31, 0.57, 10.4], [8.3, 0.85, 0.14, 2.5, 0.093, 13.0, 54.0, 0.99724, 3.36, 0.54, 10.1], [6.0, 0.31, 0.47, 3.6, 0.067, 18.0, 42.0, 0.99549, 3.39, 0.66, 11.0], [6.7, 0.32, 0.44, 2.4, 0.061, 24.0, 34.0, 0.99484, 3.29, 0.8, 11.6], [7.4, 0.36, 0.3, 1.8, 0.074, 17.0, 24.0, 0.99419, 3.24, 0.7, 11.4]]\n"
     ]
    }
   ],
   "source": [
    "dbdata = []\n",
    "\n",
    "with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD=' + sqlPassword) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT * FROM WineData\")\n",
    "        row = cursor.fetchone()\n",
    "        while row:\n",
    "            dbrow = []\n",
    "            for element in row:\n",
    "                dbrow.append(element)\n",
    "            dbdata.append(dbrow)\n",
    "            row = cursor.fetchone()\n",
    "\n",
    "print(dbdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.0], [4.666666666666667], [5.0], [6.0], [6.785714285714286], [8.0]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = {\"Content-Type\": \"application/json\",\n",
    "                   \"Authorization\": \"Bearer \" + primary_key}\n",
    "\n",
    "responses = []\n",
    "\n",
    "for inputdata in dbdata:\n",
    "    # Convert the array to JSON\n",
    "    json_data = json.dumps({\"data\": [inputdata]})\n",
    "\n",
    "    # Post to Azure (AKS)\n",
    "    response = requests.post(url=endpoint,\n",
    "                            data=json_data,\n",
    "                            headers=request_headers)\n",
    "    responses.append(response.json())\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0, 4.666666666666667, 5.0, 6.0, 6.785714285714286, 8.0\n"
     ]
    }
   ],
   "source": [
    "# outputToDB = \"\"\n",
    "\n",
    "# for value in responses:\n",
    "#     outputToDB = outputToDB + str(value[0]) + \", \"\n",
    "# outputToDB = outputToDB[:-2]\n",
    "# print(outputToDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Predicted Data into Azure SQL Server Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'aks-aml-poc-outputdb'\n",
    "\n",
    "with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD=' + sqlPassword) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        for value in responses:\n",
    "            cursor.execute(\"INSERT INTO PredictedData VALUES (\" + str(value[0]) + \");\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "4.666666666666667\n",
      "5.0\n",
      "6.0\n",
      "6.785714285714286\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "database = 'aks-aml-poc-outputdb'\n",
    "\n",
    "with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD=' + sqlPassword) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT * FROM PredictedData\")\n",
    "        row = cursor.fetchone()\n",
    "        while row:\n",
    "            for element in row:\n",
    "                print(element)\n",
    "            row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Synapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, uuid, sys\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.core._match_conditions import MatchConditions\n",
    "from azure.storage.filedatalake._models import ContentSettings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account_name = \"amlakspocdatalake\"\n",
    "storage_account_key = \"Y/AMeI4WYrOJWarsp0D/Y4K4Sn87KxDFi1zp/JITQgqODaAJG2t7m4sY3iUCzq2ONDjM73by0zb2+ASt3f+XMA==\"\n",
    "    \n",
    "try:  \n",
    "    global service_client\n",
    "\n",
    "    service_client = DataLakeServiceClient(account_url=\"{}://{}.dfs.core.windows.net\".format(\n",
    "        \"https\", storage_account_name), credential=storage_account_key)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specified container already exists.\n",
      "RequestId:f4df1d47-801e-000d-3f24-07b98c000000\n",
      "Time:2022-12-03T14:35:04.5227642Z\n",
      "ErrorCode:ContainerAlreadyExists\n",
      "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n",
      "RequestId:f4df1d47-801e-000d-3f24-07b98c000000\n",
      "Time:2022-12-03T14:35:04.5227642Z</Message></Error>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    global file_system_client\n",
    "\n",
    "    file_system_client = service_client.create_file_system(file_system=\"datalakefs\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    file_system_client.create_directory(\"my-directory\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    file_system_client = service_client.get_file_system_client(file_system=\"datalakefs\")\n",
    "\n",
    "    directory_client = file_system_client.get_directory_client(\"my-directory\")\n",
    "    \n",
    "    file_client = directory_client.get_file_client(\"wine_data.csv\")\n",
    "\n",
    "    local_file = open(\"C:\\\\Users\\\\VIGNEN\\\\Documents\\\\Azure POC\\\\aml-aks-poc\\\\jobsdata\\\\jobs_dynamic.csv\",'r')\n",
    "\n",
    "    file_contents = local_file.read()\n",
    "\n",
    "    file_client.upload_data(file_contents, overwrite=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf00967fd34e7ff43ddd408113f0923eba1ef68a4906e744cfe0dba8df86eb90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
